{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "T-TMj7ciY1R9"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amitsamria/DSpace/blob/master/module3/NLPTransformers_Mod3Demo1_Attention_ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introducing Attention\n",
        "\n",
        "Â© Data Trainers LLC. GPL v 3.0.\n",
        "\n",
        "**Author:** Axel Sirota\n",
        "\n",
        "Attention is one of the most groundbreaking ideas that revolutionized NLP and AI on the latest years. However, it is difficult to encounter a demo that is solely focused on attention... until now."
      ],
      "metadata": {
        "id": "edRPa0-vYVcA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prep"
      ],
      "metadata": {
        "id": "T-TMj7ciY1R9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "GTDmOLKh4gJy",
        "outputId": "0b076d72-de98-4767-92e0-b1b6894ee6c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'gensim'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-cda5d3b44fe3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import re\n",
        "import gensim\n",
        "from nltk.data import find\n",
        "import nltk\n",
        "\n",
        "nltk.download(\"word2vec_sample\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's define some helper functions we need:\n",
        "\n",
        "* The softmax funciton definition for Numpy arrays\n",
        "* An Embedder that transforms a list of words into its embedding representation according to `word2vec_sample` from the package `nltk`.\n",
        "\n",
        "If you are unfamiliar with these concepts you are welcome to come to my other course **Implement Natural Language Processing for Word Embedding**"
      ],
      "metadata": {
        "id": "hw0pwirWXuYA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x, axis=0):\n",
        "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
        "    return np.exp(x) / np.sum(np.exp(x))"
      ],
      "metadata": {
        "id": "Cc9Eevlj573a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_word2vec_embedding(words):\n",
        "    \"\"\"\n",
        "    Function that takes in a list of words and returns a list of their embeddings,\n",
        "    based on a pretrained word2vec encoder.\n",
        "    \"\"\"\n",
        "    word2vec_sample = str(find(\"models/word2vec_sample/pruned.word2vec.txt\"))\n",
        "    model = gensim.models.KeyedVectors.load_word2vec_format(\n",
        "        word2vec_sample, binary=False\n",
        "    )\n",
        "\n",
        "    output = []\n",
        "    words_pass = []\n",
        "    for word in words:\n",
        "        try:\n",
        "            output.append(np.array(model.word_vec(word)))\n",
        "            words_pass.append(word)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    embeddings = np.array(output)\n",
        "    del model  # free up space again\n",
        "    return embeddings, words_pass\n"
      ],
      "metadata": {
        "id": "bd99pIV059c5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention 101: Dot product Attention"
      ],
      "metadata": {
        "id": "NTtHbfE1ZkCk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The idea behind attention is simple, if you take any word, like `Apple`, its meaning will change with respect with the other words in the sentence. For example below, In the first sentence Apple refers to the company and has strong relationship with coding and computer; on the second one refers to the fruit and therefore at most it would have relationship with eating, but not coding."
      ],
      "metadata": {
        "id": "fzHIp_SzdYi6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<figure>\n",
        "<center>\n",
        "<img src='https://www.dropbox.com/s/91xzqre8dpvxrux/sentence.png?raw=1' alt=\"drawing\" width=\"350\" />\n",
        "<figcaption>Words relevance change with context</figcaption></center>\n",
        "</figure>"
      ],
      "metadata": {
        "id": "bVj0D19CcHCk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  What I just spoke, is known as **Cross Attention**, because you will calculate the relationship of one word with respect to **all** the others in the sentence. In an image it would be:\n",
        "\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src='https://www.dropbox.com/s/ahn8ogriuzasa9a/attention_in_detail.png?raw=1'  />\n",
        "<figcaption>Attention</figcaption></center>\n",
        "</figure>"
      ],
      "metadata": {
        "id": "ZMJ6SbhaeeO5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pu-KMY65gLvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In code it is even easier, don't worry about the image above it will make sense as we evolve through the course. The really important part is the following:\n",
        "\n",
        "$$\n",
        "a_{ij} = f(h_i, s_{j})\n",
        "$$\n",
        "\n",
        "Where $a_{i,j}$ stands for the alignment of the word `h_i` with the output word `s_j`. The alignment may sound fancy, but it simply means how strongly connected those 2 words are in that sentence, like the Apple example!\n",
        "\n",
        "\n",
        "The key is that the function $f$ can be anything. In the original paper, and the one we are implementing now it is the dot product, which you have probably seen before, and if not check the course I referenced before,  **Implement Natural Language Processing for Word Embedding**:\n",
        "\n",
        "$$\n",
        "a_{i,j} = dot product(h_i, s_j) = h_i^T*s_j\n",
        "$$\n",
        "\n",
        "So this means that for a given initial word, which is a row in the matrix we created, we have a Tensor of how aligned it is with that output word; we call that Tensor `c_k` or context vector.\n",
        "\n",
        "And here comes the important stuff number 2, which is we take softmax to obtain weights, those wieghts will tell me for that input word how much weight (and importance) I should put into any output word. That is the attention matrix.\n",
        "\n",
        "$$\n",
        "z_j = softmax_k(c_{j,k})\n",
        "$$\n",
        "\n",
        "If we multiply this with the context vector of an encoder we have an empowered context memory tensor that can be fed into the decoder, as it is done in Transformers. We will implement all of this alongside this module"
      ],
      "metadata": {
        "id": "YcYDNPZsfqtv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dot_product_attention(hidden_states, previous_state):\n",
        "\n",
        "    # [T,d]*[d,N] -> [T,N]\n",
        "    scores = np.matmul(previous_state, hidden_states.T)\n",
        "    w_n = softmax(scores)\n",
        "\n",
        "    # [T,N]*[N,d] -> [T,d]\n",
        "    c_t = np.matmul(w_n, hidden_states)\n",
        "\n",
        "    return w_n, c_t"
      ],
      "metadata": {
        "id": "FZGYeCxV5_nA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will use a helper function that will plot those attention weights I told you about"
      ],
      "metadata": {
        "id": "ETA3QPEvf__v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_attention_weight_matrix(weight_matrix, x_ticks, y_ticks):\n",
        "    \"\"\"Function that takes in a weight matrix and plots it with custom axis ticks\"\"\"\n",
        "    plt.figure(figsize=(15, 7))\n",
        "    ax = sns.heatmap(weight_matrix, cmap=\"Blues\")\n",
        "    plt.xticks(np.arange(weight_matrix.shape[1]) + 0.5, x_ticks)\n",
        "    plt.yticks(np.arange(weight_matrix.shape[0]) + 0.5, y_ticks)\n",
        "    plt.title(\"Attention matrix\")\n",
        "    plt.xlabel(\"Attention score\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "vTZbCxr26Bgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing it out"
      ],
      "metadata": {
        "id": "qnUjZSH1l_BQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try with some words related to royalty and some related to food:"
      ],
      "metadata": {
        "id": "J4bXyz_PjNmV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = [\"king\", \"queen\", \"royalty\", \"food\", \"apple\", \"pear\", \"computers\"]\n",
        "word_embeddings, words = get_word2vec_embedding(words)\n",
        "weights, _ = dot_product_attention(word_embeddings, word_embeddings)\n",
        "plot_attention_weight_matrix(weights, words, words)"
      ],
      "metadata": {
        "id": "uq2d3BZj6C5j",
        "outputId": "f2201599-d973-4e4c-8a8c-3e67a33ded2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'get_word2vec_embedding' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-8ef5f6d97594>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"king\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"queen\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"royalty\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"food\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"apple\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pear\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"computers\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mword_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_word2vec_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdot_product_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplot_attention_weight_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'get_word2vec_embedding' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, this was successful! We could detect the relationships between apple, pear and a little less food; aas one cluster. Then another cluster of the royalty, and finally commputers alone, so it detected what it is supposed to! In the next demo we will implement other forms of attention, ie: changing that function `f`"
      ],
      "metadata": {
        "id": "Gsd6Ynvfj5N9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bJzGDX_d6EZb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}